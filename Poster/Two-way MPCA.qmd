---
title: Regularized Multivariate Two-way Functional Principal Component Analysis
format:
  poster-typst: 
    size: "36x24"
    poster-authors: "Mobina Pourmoshir, Dr. Mehdi Maadooliat"
    departments: "Department of Mathematical and Statistical Sciences"
    institution-logo: "./images/MU.png"
    footer-text: "CSSRFP 2025"
    footer-url: "https://github.com/mobinapourmoshir/ReMPCA"
    footer-emails: "mobina.pourmoshir@marquette.edu"
    footer-color: "ebcfb2"
bibliography: ref.bib
csl: apa.csl
citeproc: true
---

# Background

- **Functional data are ubiquitous.**  
  Modern sensors yield curves, images, and surfaces observed over time/space.  
  Functional PCA (FPCA) summarizes such data with a few principal functions for interpretation and modeling [@ramsay2005functional].

- **Extensions exist but are isolated.**
  - *Smoothed FPCA*: roughness penalties produce smoother, less noisy components [@silverman1996smoothed; @huang2008functional].
  - *Sparse FPCA*: sparsity zeros out unimportant regions, improving interpretability [@huang2008sparse; @SparsePCA2020].
  - *Multivariate FPCA (MFPCA)*: captures shared variation across multiple functional variables [@Happ_2018].
  - *Two-way functional data* (e.g., time × space): require structure in *both* domains.

- **Limitations.**  
  Classical FPCA is noise-sensitive and can yield rough, dense patterns; many methods address *either* smoothness *or* sparsity, or are limited to univariate data.

- **Motivation.**  
  Develop a regularized FPCA framework that (i) handles **multivariate** and **two-way** functional structures,  
  (ii) imposes **smoothness** (noise reduction) and **sparsity** (feature selection) *simultaneously* on scores and loadings,  
  and (iii) yields low-rank, interpretable, and stable components.  
  Recent work (e.g., ReMFPCA) points in this direction but leaves room for a unified treatment and broader applicability [@haghbin2023regMFPCA].



# Methodology
### Multivariate FPCA Formulation

Concatenate $p$ functional variables into $\mathbf{X}\in\mathbb{R}^{n\times M}$, where $M = \sum_{i=1}^p m_i$.  
We estimate a rank-one structure with penalties:

$$
\min_{u,v}\;\|\mathbf{X}-uv^\top\|_F^2
+ \alpha\, v^\top \boldsymbol{\Omega} v
+ p_\gamma(v),
$$

where $\boldsymbol{\Omega} = \mathrm{diag}(\Omega_1,\ldots,\Omega_p)$ encodes **roughness** and $p_\gamma(\cdot)$ induces **sparsity** (soft, hard, or SCAD) [@huang2008functional; @huang2008sparse; @SparsePCA2020].


### Sequential Power Algorithm

Let $S(\alpha) = (I + \alpha \boldsymbol{\Omega})^{-1}$. Iterate:

1. **Initialize:** $v$ via rank-one SVD of $\mathbf{X}$.
2. **Repeat:**  
   - $u \leftarrow \mathbf{X}v$  
   - $v \leftarrow S(\alpha)\,h_\gamma(\mathbf{X}^\top u)$  
   - $v \leftarrow v / \|v\|$
3. **Deflate:** $\mathbf{X} \leftarrow \mathbf{X} - \sigma uv^\top$ to extract additional components.

**Tuning:**  
Choose $\gamma$ by $K$-fold cross-validation.  
Choose $\alpha$ by generalized cross-validation (GCV):

$$
\mathrm{GCV}(\alpha)
= \frac{\| (I - S(\alpha))(\mathbf{X}^\top u)\|^2 / M}
{ \left( 1 - \tfrac{1}{M} \operatorname{tr} S(\alpha) \right)^2 }.
$$



# Two-way Regularized MFPCA

- **Two-way functional data:**  
  Two-way functional data consist of a data matrix whose row and column domains are both structured. Classical FPCA focuses on one domain and penalizes only one set of components, often ignoring structure in the second direction.


- **Framework & Penalty:**  
  $$
  \min_{u,v} \|\pmb X - uv^\top\|_F^2 + 
    \sum_j^J \mathcal P_j^{[\theta]}(u,v)
  $$
- where $J$ is the number of penalty components, and $\theta$ is the vector of all tuning parameters. The composite penalty $\sum_{j=1}^J \mathcal{P}_j^{(\theta)}(u,v)$ lets us mix regularizers, e.g., smoothness with $\theta=(\alpha_u,\boldsymbol{\alpha}_v)$ and sparsity with $\theta=(\gamma_u,\boldsymbol{\gamma}_v)$ (controlling sparsity), and can include other structures as needed.



- **Sequential Power Algorithm:**  
  1. Initialize $u, v$ using rank-one SVD of $\mathbf{X}$.  
  2. Update $u$ with smoothing and sparsity transformations:  
     $u \leftarrow S_u^{[\alpha_u]} h_u^{[\gamma_u]}(\mathbf{X}v)$  
  3. Update $v$ similarly:  
     $v \leftarrow S_v^{[\boldsymbol{\alpha}_v]} h_v^{[\boldsymbol{\gamma}_v]}(\mathbf{X}^\top u)$  
  4. Normalize $v$ and deflate $\mathbf{X}$ to extract further components.

- **Tuning:**  
    - $\alpha_u$, $\boldsymbol{\alpha}_v$ → control **smoothness** of scores and loadings.  
    - $\gamma_u$, $\boldsymbol{\gamma}_v$ → control **sparsity** of scores and loadings.
  - **Strategy (Conditional Tuning):**  
    1. Initialize with no penalties.  
    2. Use **cross-validation (CV)** to tune sparsity parameters.  
    3. Use **generalized cross-validation (GCV)** to tune smoothness parameters.  
    4. Alternate steps 2–3 until convergence for an optimal balance of smoothness and sparsity.



```{=typst}
#grid(
  columns: 4,
  gutter: 2pt,  // reduce spacing between images

  // Row 1
  image("images/True.jpg", width: 2.5in),
  image("images/svd_Reconstructed.jpg", width: 2.5in),
  image("images/Lasso_reconstructed.jpg", width: 2.5in),
  image("images/u1plots.jpg", width: 2.5in),

  // Row 2
  image("images/NoisyChessboard.jpg", width: 2.5in),
  image("images/smooth_reconstructed.jpg", width: 2.5in),
  image("images/pic.compress.jpg", width: 2.5in),
  image("images/v1plots.jpg", width: 2.5in),
)
```


# Conclusion & Future Work
- **Comprehensive Framework:** ReMPCA extends PCA to curves, images, and functional data, combining smoothness (denoising, interpretability) and sparsity (feature selection) for structured, low-rank components.
- **Methodology:** Penalized SVD with roughness and sparsity penalties applies regularization to both scores ($u$) and loadings ($v$), tuned via GCV and CV.
- **Results:** Two-way regularization improves reconstruction and interpretability, outperforming one-way methods across simulated and real datasets.
- **Software:** Implemented in the ReMPCA R package with automated tuning, diagnostics, and visualization tools.
- **Future Work:** 
  - Hybrid data integration: joint analysis of scalar, functional, and imaging data.
  - Advanced models: kernel FPCA and neural-network-based nonlinear extensions.
  - Applications: neuroimaging, personalized medicine, and environmental monitoring.

# References
```{=typst}
#set text(size: 12pt)
```

