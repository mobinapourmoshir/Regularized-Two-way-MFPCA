---
title: Regularized Multivariate Two-way Functional Principal Component Analysis
format:
  poster-typst: 
    size: "36x24"
    poster-authors: "Mobina Pourmoshir, Dr. Mehdi Maadooliat"
    departments: "Department of Mathematical and Statistical Sciences"
    institution-logo: "./images/MU.png"
    footer-text: "Marquette University"
    footer-url: "https://github.com/mobinapourmoshir/ReMPCA"
    footer-emails: "mobina.pourmoshir@marquette.edu"
    footer-color: "ebcfb2"
bibliography: ref.bib
csl: apa.csl
citeproc: true
---

# Background

- **Functional data are ubiquitous.**  
  Modern sensors yield curves, images, and surfaces observed over time/space.  
  Functional PCA (FPCA) summarizes such data with a few principal functions for interpretation and modeling [@ramsay2005functional].

- **Extensions exist but are isolated.**
  - *Smoothed FPCA*: Smoother components [@huang2008functional].
  - *Sparse FPCA*: Improving interpretability [@huang2008sparse; @SparsePCA2020].
  - *Multivariate FPCA (MFPCA)*: Multiple functional variables [@Happ_2018].
  - *Two-way functional data*: require structure in *both* domains.

- **Limitations.**  
  Classical FPCA is noise-sensitive and can yield rough, dense patterns; many methods address *either* smoothness *or* sparsity, or are limited to univariate data.

- **Motivation.**  
  Develop a regularized FPCA framework that (i) handles **multivariate** and **two-way** functional structures,  (ii) imposes **smoothness** (noise reduction) and **sparsity** (feature selection) *simultaneously* on scores and loadings,  and (iii) yields low-rank, interpretable, and stable components.  Recent work points in this direction but leaves room for a unified treatment and broader applicability [@haghbin2023regMFPCA].



# Methodology
### Multivariate FPCA

Concatenate $p$ functional variables, where the $i$-th variable is observed on $m_i$ grid points, into $\mathbf{X} = \big[\,{X_1}\quad X_2 \; \cdots \; {X_p}\,\big] \in\mathbb{R}^{n\times M}$, where $M = \sum_{i=1}^p m_i$:

$$
\mathbf{X} =
\begin{bmatrix}
{x_{11}(t_{11})} & {\cdots} & {x_{11}(t_{1m_1})} & \cdots & {x_{1p}(t_{p1})} & {\cdots} & {x_{1p}(t_{p m_p})} \\
{\vdots} & {\ddots} & {\vdots} & \ddots & {\vdots} & {\ddots} & {\vdots} \\
{x_{n1}(t_{11})} & {\cdots} & {x_{n1}(t_{1m_1})} & \cdots & {x_{np}(t_{p1})} & {\cdots} & {x_{np}(t_{p m_p})}
\end{bmatrix}
$$


We estimate a rank-one structure with penalties:

$$
\min_{u,v}\;\|\mathbf{X}-uv^\top\|_F^2
+ \alpha\, v^\top \boldsymbol{\Omega} v
+ p_\gamma(v),
$$

where $\boldsymbol{\Omega} = \mathrm{diag}(\Omega_1,\ldots,\Omega_p)$ encodes **roughness** and $p_\gamma(\cdot)$ induces **sparsity** (soft, hard, or SCAD) [@huang2008functional; @huang2008sparse; @SparsePCA2020].


### Sequential Power Algorithm

Let $S(\alpha) = (I + \alpha \boldsymbol{\Omega})^{-1}$. Iterate:

1. **Initialize:** $v$ via rank-one SVD of $\mathbf{X}$.
2. **Repeat:**  
   - $u \leftarrow \mathbf{X}v$  
   - $v \leftarrow S(\alpha)\,h_\gamma(\mathbf{X}^\top u)$  
   - $v \leftarrow v / \|v\|$

**Tuning:**  
Choose $\gamma$ by $K$-fold cross-validation.  
Choose $\alpha$ by generalized cross-validation (GCV): $\mathrm{GCV}(\alpha) = \frac{\| (I - S(\alpha))(\mathbf{X}^\top u)\|^2 / M}{ \left( 1 - \tfrac{1}{M} \operatorname{tr} S(\alpha) \right)^2}.$



# Two-way Regularized MFPCA

- **Two-way functional data:**  
  Two-way functional data consist of a data matrix whose row and column domains are both structured. Classical FPCA focuses on one domain and penalizes only one set of components, often ignoring structure in the second direction.


- **Framework & Penalty:**  
  $$
  \min_{u,v} \|\pmb X - uv^\top\|_F^2 + 
    \sum_j^J \mathcal P_j^{[\theta]}(u,v)
  $$
- where $J$ is the number of penalty components, and $\theta$ is the vector of all tuning parameters. The composite penalty $\sum_{j=1}^J \mathcal{P}_j^{(\theta)}(u,v)$ lets us mix regularizers, e.g., smoothness with $\theta=(\alpha_u,\boldsymbol{\alpha}_v)$ and sparsity with $\theta=(\gamma_u,\boldsymbol{\gamma}_v)$ (controlling sparsity), and can include other structures as needed.



- **Sequential Power Algorithm:**  
  1. Initialize $u, v$ using rank-one SVD of $\mathbf{X}$.  
  2. Update $u$ with smoothing and sparsity transformations:  
     $u \leftarrow S_u^{[\alpha_u]} h_u^{[\gamma_u]}(\mathbf{X}v)$  
  3. Update $v$ similarly:  
     $v \leftarrow S_v^{[\boldsymbol{\alpha}_v]} h_v^{[\boldsymbol{\gamma}_v]}(\mathbf{X}^\top u)$  
  4. Normalize $v$ and deflate $\mathbf{X}$ to extract further components.

- **Tuning:**  
  - $\alpha_u$, $\boldsymbol{\alpha}_v$ → control **smoothness** of scores and loadings. 
  - $\gamma_u$, $\boldsymbol{\gamma}_v$ → control **sparsity** of scores and loadings.
  - Conditional tuning alternates CV for sparsity and GCV for smoothness to achieve an optimal balance.



```{=typst}
#grid(
  columns: 4,
  gutter: 2pt,  // reduce spacing between images

  // Row 1
  image("images/True.jpg", width: 2.5in),
  image("images/svd_Reconstructed.jpg", width: 2.5in),
  image("images/Lasso_reconstructed.jpg", width: 2.5in),
  image("images/u1plots.jpg", width: 2.5in),

  // Row 2
  image("images/NoisyChessboard.jpg", width: 2.5in),
  image("images/smooth_reconstructed.jpg", width: 2.5in),
  image("images/pic.compress.jpg", width: 2.5in),
  image("images/v1plots.jpg", width: 2.5in),
)
```


# Conclusion & Future Work
- **Comprehensive Framework:** ReMPCA extends PCA to curves, images, and functional data, combining smoothness (denoising, interpretability) and sparsity (feature selection) for structured, low-rank components.
- **Methodology:** Penalized SVD with roughness and sparsity penalties applies regularization to both scores ($u$) and loadings ($v$), tuned via GCV and CV.
- **Results:** Two-way regularization improves reconstruction and interpretability, outperforming one-way methods across simulated and real datasets.
- **Software:** Implemented in the ReMPCA R package with automated tuning, diagnostics, and visualization tools.
- **Future Work:**
  - Hybrid data integration: joint analysis of scalar, functional, and imaging data.
  - Advanced models: kernel FPCA and neural-network-based nonlinear extensions.
  - Applications: neuroimaging, personalized medicine, and environmental monitoring.
  
```{=typst}
#grid(
  columns: 2,
  gutter: 1in, // increase this for a bigger horizontal gap
  row-gutter: 0pt, // optional: remove vertical gaps

  image("images/funcTrue.svg", width: 5in),
  image("images/RegTrue.svg", width: 5in),
  image("images/funcNoisy.svg", width: 5in),

  image("images/RegNoisy.svg", width: 5in),
  image("images/funcReco.svg", width: 5in),
  image("images/RegReco.svg", width: 5in),
)
```

# References
```{=typst}
#set text(size: 9pt)
```

