---
title: "Regularized Multivariate Two-way Functional Principal Component Analysis"
author:
    name: "Mobina Pourmoshir"
    email: "mobina.pourmoshir@marquette.edu"
institute: |
  Department of Mathematical and Statistical Sciences
  \par
  Marquette University
#date: "Sep 2025"
editor: visual

format:
  beamer:
    theme: CambridgeUS
    colortheme: crane
    aspectratio: 169
    slide-level: 2
    auto-play-media: true
    pdf-engine: pdflatex
    cite-method: natbib

link-citations: true



header-includes:
  - |
    \usepackage{xcolor}
    \usepackage{subcaption}
    \usepackage{amsmath, amssymb}
    \usepackage{graphicx}
    \usepackage{amsmath,amssymb,mathtools}
    \usepackage{algorithm}
    \usepackage{natbib}
    \AtBeginEnvironment{CSLReferences}{\tiny}
    \makeatletter
    % Load enumitem only for non-beamer docs, so <+-> works in beamer
    \@ifclassloaded{beamer}{}{%
      \usepackage{enumitem}%
    }
    \makeatother
    
    \usepackage{multimedia}
    % ---- Fancy glossy numbered label (TikZ) ----
    \usepackage{tikz}
    \usetikzlibrary{shadings}
    \definecolor{darknavy}{RGB}{10,38,71} % already in your file; harmless to repeat
    % Make \shorthandoff a no-op if babel isn't loaded
    \providecommand{\shorthandoff}[1]{}

    \newcommand*\fancynum[1]{%
      \begingroup
      \shorthandoff{:;!?}%
      \begin{tikzpicture}[baseline=(c.base)]
        % glossy sphere-style circle
        \shade[ball color=darknavy] (0,0) circle (1.35ex);
        % white number on top, bold & tiny
        \node[font=\scriptsize\bfseries, text=white] (c) at (0,0) {#1};
      \end{tikzpicture}%
      \endgroup
    }
            
    \AtBeginDocument{%
      \author[Mobina Pourmoshir]{Mobina Pourmoshir\\\small\texttt{mobina.pourmoshir@marquette.edu}}%
    }
    \setbeamertemplate{section page}[none]
    \AtBeginSection[]{}
    \definecolor{darknavy}{RGB}{0,0,102}
    \definecolor{MUgold}{HTML}{FFC72C}
    \definecolor{TitleRed}{HTML}{B90E0A}
    \definecolor{TitleBlue}{HTML}{1520A6}
    
    \setbeamercolor{title}{fg=darknavy}
    \setbeamercolor{palette primary}{bg=MUgold, fg=black}
    \setbeamercolor{palette secondary}{bg=MUgold, fg=black}
    \setbeamercolor{palette tertiary}{bg=MUgold, fg=black}
    \setbeamercolor{palette quaternary}{bg=MUgold, fg=black}
    \setbeamercolor{author in head/foot}{bg=MUgold, fg=black}
    \setbeamercolor{title in head/foot}{bg=MUgold,  fg=black}
    \setbeamercolor{date in head/foot}{bg=MUgold,   fg=black}
    \titlegraphic{\vspace{-6mm}\includegraphics[width=0.18\textwidth]{images/MU.png}}
    \setbeamertemplate{footline}{%
      \leavevmode
      \hbox{%
        \begin{beamercolorbox}[wd=.33\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}Mobina Pourmoshir\end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.34\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}Regularized Multivariate Two-way FPCA\end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.33\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}\end{beamercolorbox}%
      }
      \vskip0pt
    }
    \setbeamertemplate{headline}{%
      \begin{beamercolorbox}[wd=\paperwidth,ht=2.6ex,dp=1ex,left]{palette primary}
        \hspace*{1ex}\insertsectionnavigationhorizontal{\paperwidth}{}{}
      \end{beamercolorbox}
    }
    
    
    \setbeamertemplate{section page}[none]
    \AtBeginSection[]{}
    \definecolor{darknavy}{RGB}{0,0,102}
    \setbeamercolor{title}{fg=darknavy}
    \definecolor{DarkGreen}{HTML}{028A0F}
    \definecolor{Clay}{HTML}{FC6A03}
    \definecolor{MUgold}{HTML}{FFC72C}     % gold
    \setbeamercolor{title}{fg=darknavy} % Make title dark navy
    
    
    % --- SOLID (NON-GRADIENT) bars in header/footer ---
    % Make all palette stripes the SAME gold so there's no light-to-dark effect
    \setbeamercolor{palette primary}{bg=MUgold, fg=black}
    \setbeamercolor{palette secondary}{bg=MUgold, fg=black}
    \setbeamercolor{palette tertiary}{bg=MUgold, fg=black}
    \setbeamercolor{palette quaternary}{bg=MUgold, fg=black}
    % And match the footline boxes
    \setbeamercolor{author in head/foot}{bg=MUgold, fg=black}
    \setbeamercolor{title in head/foot}{bg=MUgold,  fg=black}
    \setbeamercolor{date in head/foot}{bg=MUgold,   fg=black}
    \setbeamercolor{section in head/foot}{fg=black,bg=MUgold}
    \setbeamercolor{section in head/foot shaded}{fg=black!20,bg=MUgold}
    

    %==================== header & footer (default) ====================
    \setbeamertemplate{footline}{%
      \leavevmode%
      \hbox{%
        \begin{beamercolorbox}[wd=.33\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}Mobina Pourmoshir\end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.34\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}Regularized Multivariate Two-way FPCA\end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.33\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}\insertframenumber{} / \inserttotalframenumber\hspace*{2ex}\end{beamercolorbox}%
      }%
      \vskip0pt%
    }

    \setbeamertemplate{headline}{%
      \begin{beamercolorbox}[wd=\paperwidth,ht=2.6ex,dp=1ex,left]{palette primary}
        \hspace*{1ex}\insertsectionnavigationhorizontal{\paperwidth}{}{}
      \end{beamercolorbox}%
    }
    \setbeamercolor{section in head/foot}{fg=black,bg=MUgold}
    \setbeamercolor{section in head/foot shaded}{fg=black!20,bg=MUgold}
    
     % --- Move the logo UP on the title slide ---
    % Reduce vertical space before the logo (negative vspace pulls it upward)
    \titlegraphic{\vspace{-6mm}\includegraphics[width=0.18\textwidth]{images/MU.png}}

    % (Optional) remove title-box drop shadow if you dislike it
    % \useoutertheme{default}
    \newcommand{\SimResultsTitle}{%
      \only<1>{Simulation Results (SVD)}%
      \only<2>{Simulation Results (Smoothness and Sparsity on v)}%
      \only<3>{Simulation Results (Smoothness and Sparsity on u)}%
      \only<4>{Simulation Results (Two-Way Smoothness)}%
      \only<5>{Simulation Results (Two-Way Sparsity)}%
      \only<6>{Simulation Results (Two-way Sparsity and Smoothness)}%
    }
---


# Outline
            
\setbeamertemplate{enumerate item}{\fancynum{\insertenumlabel}}

\begin{enumerate}
  \setlength{\itemsep}{0.9em}   % vertical spacing between items
  \setlength{\leftmargini}{2.2em} % left margin for level 1 enumerate
  \item \hyperlink{sec:intro}{Introduction \& Background}
  \item \hyperlink{sec:svd}{A SVD Approach for Regularized Multivariate FPCA}
  \item \hyperlink{sec:two_way}{Two-way Regularized Multivariate FPCA}
  \item \hyperlink{sec:conclusion}{Conclusion \& Future Work}
\end{enumerate}



# Introduction {auto-animate="true"}
\hypertarget{sec:intro}{}

\small\normalfont
\begin{columns}[T,onlytextwidth]
  % -------- Left: bullets (come one by one) ----------
  \begin{column}{0.62\textwidth}
  {\small\normalfont
  \begin{itemize}
    \item<1-> \textbf{Functional data} are observations that change continuously over a domain (like time, space, or wavelength) and are often visualized as curves, trajectories, or functions rather than isolated points.
    \item<2-> In practice, these data are often recorded at discrete time points or grid locations, even though they originate from continuous processes in areas like engineering, finance, environmental science, and healthcare.
    \item<3-> \textbf{Functional Data Analysis (FDA)} is a statistical framework that treats these observations as realizations of smooth underlying functions, allowing for more accurate modeling and interpretation of continuous processes.
  \end{itemize}}
  \end{column}

  % -------- Right: images ----------
  \begin{column}{0.38\textwidth}
    \vspace{-1.5em}
    \only<1->{\centering\includegraphics[height=0.35\textheight]{images/FDA2.png}\par}
    \only<2->{\centering\includegraphics[height=0.35\textheight]{images/FDA1.png}\par}
  \end{column}
\end{columns}



## Functional Principal Component Analysis {auto-animate="true"}

- **Functional PCA (FPCA):** An extension of classical PCA for dimension reduction and uncovering hidden patterns in functional data; it identifies orthogonal functions that capture the main sources of variation, preserving the most important information. [@ramsay2005functional].

- **Extensions of FPCA:**
  - \textcolor{TitleRed}{Smoothed FPCA}: Adds roughness penalties for smoothness [@silverman1996smoothed; @huang2008functional].
  - \textcolor{TitleBlue}{Sparse FPCA}: Enforces sparsity for interpretability [@huang2008sparse; @SparsePCA2020].
  - Multivariate FPCA (MFPCA): Extends FPCA to multivariate functions [@silverman1996smoothed; @Happ_2018].
  - Regularized MFPCA: Penalties improve estimation & interpretability [@haghbin2023regMFPCA].

- **Impact:** More adaptable, robust, and applicable across diverse scientific and business problems.


## Two-way Functional Principal Component Analysis {auto-animate="true"}

::: incremental
- **Two-way functional data:** Observations vary along two domains (e.g., time $\times$ space, time $\times$ frequency), with applications in climate, neuroscience, finance, public health, and marketing.

    ```{=latex}
    % shows ONLY while the first bullet is active
    \vspace{-1.5em}
    \only<1>{\centering\includegraphics[width=0.3\linewidth]{images/NDVI.png}\par}
    \vspace{1.5em}
    ```
:::

::: {.fragment style="height: 0.6em"}
::: 

::: incremental
- **Extension of FPCA:** Huang [@twoway_huang] applied regularization to both left and right singular vectors in SVD.
:::

::: {.fragment style="height: 0.6em"}
:::

- **Practical challenges:**
  - Data observed on discrete grids (minutes, hours, days).
  - Issues: measurement noise, irregular sampling, missing data, loss of smoothness.

::: {.fragment style="height: 0.6em"}
:::

- **Proposed framework:**
  - Unified FPCA for two-way multivariate functional data.
  - \textcolor{TitleRed}{Smoothness} penalties preserve functional structure.
  - \textcolor{TitleBlue}{Sparsity} penalties enhance interpretability.
  - Effective for dimension reduction in complex datasets.
  


## Foundations of FPCA through Minimizing Reconstruction Error
- **Goal:** Identify functional directions that maximize variance (low-rank approximation of functional data).  For functional data $X \in \mathbb{R}^{n \times m}$ contains the discretized functional observations (rows correspond to subjects, columns to grid points), $v\in\mathbb{R}^m$ represents the estimated principal component (function), and $u\in\mathbb{R}^n$ denotes the associated principal component scores.

- **Reconstruction problem:** 
    ```{=latex}
    $$
    \min_{u,v} \; \|X - uv^\top\|_F^2 = \operatorname{tr}\{(X - uv^\top)(X - uv^\top)^\top\},
    $$
    ```
- **Optimization steps:**
    ```{=latex}
    $$
      \vspace{-1.5em}
      \text{Fix } v:\; u = \frac{Xv}{v^\top v} 
      \qquad \text{and}  \qquad
      \text{Fix } u:\; v = \frac{X^\top u}{u^\top u}
      \vspace{1.5em}
    $$
    ```


## Extensions of FPCA via Regularization

- **Goal:** Balance **variance explanation**, **smoothness**, and **interpretability**. 

::: {.fragment style="height: 0.5em"}
:::

- Reformulate FPCA as a **penalized low-rank approximation** problem:  
  $$
  \min_{u,v} \|X - uv^\top\|_F^2 + \mathcal{P}(u,v)
  $$

- Two directions:  
  - **\textcolor{TitleRed}{Smooth FPCA:}** adds roughness penalty on functions.  
  - **\textcolor{TitleBlue}{Sparse FPCA:}** adds sparsity penalty on loadings.  

::: {.fragment style="height: 0.5em"}
:::

- **Algorithms:** Based on iterative **power method** and **thresholding** updates.  



## Smooth Functional PCA
- Problem setup [@huang2008functional]:  
  $$
  \min_{u,v} \|X - uv^\top\|_F^2 + \color{TitleRed} {\alpha\, u^\top u \, v^\top \Omega v}
  $$
  - $X \in \mathbb{R}^{n \times p}$: discretized functional data.  
  - $u \in \mathbb{R}^n$: scores.  
  - $v \in \mathbb{R}^p$: loading function.  
  - $\Omega$: roughness penalty matrix (e.g., integrated squared 2nd derivative). 
  - $\alpha$: tuning parameter

::: incremental 
- A power algorithm is defined to compute the PCs while incorporating smoothness penalty. 
- Consider the SVD of $X$ as $X = UDV^\top$, where $U$ and $V$ have orthonormal columns and $D$ is diagonal with ordered singular values.  In particular, for $X = u d v^\top$, $v$ is the first principal component and $u = u d$ gives the associated scores.   With these representations, the power algorithm (described below) converges quickly, typically in only a few iterations.
:::

## Power Algorithm

::: incremental 
```{=latex}
    \begin{block}{Algorithm: Penalized Power Iteration}
\begin{enumerate}
  \item Initialize $v$.
  \item Repeat until convergence:
    \begin{enumerate}
      \item $u \leftarrow Xv$
      \item $v \leftarrow \textcolor{TitleRed}{(I + \alpha\Omega)^{-1} }X^\top u$
      \item $v \leftarrow \dfrac{v}{\lVert v \rVert}$ % use \frac if you didn't load amsmath
    \end{enumerate}
  \item Update $X \leftarrow X - \sigma\, u v^\top$ and proceed to next component.
\end{enumerate}
\end{block}
```

:::

::: incremental
- For notational convenience, we define $\color{TitleRed}{S(\alpha) = (I +\alpha\Omega)^{-1}}$ $\in \mathbb{R}^{m\times m}$, which simplifies expressions involving regularization. The penalty matrix $\Omega$ is set up so that larger values of the quadratic form ${v}^\top {\Omega} {v}$ mean rougher functions. This means that it penalizes functions that change quickly between time points.
::: 

## Tunning Smoothness Parameters
::: incremental
- To select the optimal tuning parameters $\alpha$ efficiently, one can use a traditional Cross-Validation (CV) criterion and a computationally efficient closed-form Generalized Cross-Validation (GCV) criterion:

```{=latex}
  \begin{equation*} 
	\text{CV}(\alpha) = \frac{1}{m} \sum_{j=1}^m \frac{\left[\{ ({I} - {S}(\alpha)) ({X}^T {u})\}_{jj} \right]^2}{\left(1 -   \{{S}(\alpha)\}_{jj}\right)^2},
\end{equation*}

\begin{flushleft}
\qquad \quad where $\{\cdot\}_{jj}$ denotes the $j$-th diagonal element.
\end{flushleft}

\begin{equation*} 
	\textcolor{TitleRed}{\text{GCV}(\alpha) = \frac{1}{m} \frac{\Vert ({I} - {S}(\alpha)) ({X}^T {u}) \Vert^2}{\left(1 - \frac{1}{m} \operatorname{tr} \{{S}(\alpha)\}\right)^2}}.
\end{equation*}
```
:::


## Sparse Functional PCA

- *Standard FPCA loadings are dense, involving linear combinations of all grid points → hard to interpret.*  
  **Sparsity** highlights only the most relevant features, thereby enhancing interpretability.
- *Dense loadings capture noise → unstable components.*  
  **Sparsity** filters out uninformative variation, yielding more robust principal components, reducing dimensionality, and facilitating interpretation.
- *All grid points contribute equally → no feature selection.*   
  **Sparsity** acts as an inherent feature selector, directing attention to key time points, with most entries reduced to zero while only a few contribute meaningfully to the structure.
- Sparse FPCA formulation [@huang2008sparse]:  
    ```{=latex}
    \vspace{-1em}
    \begin{equation}\label{eq:sparse}
    \min_{u,v} \|X - uv^\top\|_F^2 + \textcolor{TitleBlue}{p_\gamma(v)}
    \end{equation}
    \vspace{-1.5em}
    \begin{flushleft}
    \qquad \quad where $\textcolor{TitleBlue}{p_\gamma(v)}$ is a sparsity-inducing penalty. 
    \end{flushleft}
    ```
   


## Sparsity penalties

\scriptsize
- **Soft-thresholding (Lasso):**  
  $$
  p^{\text{soft}}_\gamma(|\theta|) = 2 \gamma |\theta|, 
  \;\xrightarrow[\text{minimizer}]{}\;
  h_\gamma^{\text{soft}}(y) = \operatorname{sign}(y)(|y|-\gamma)_{+}
  $$

- **Hard thresholding:**  
  $$
  p^{\text{hard}}_\gamma(|\theta|) = \gamma^2 I(|\theta| \neq 0), 
  \;\xrightarrow[\text{minimizer}]{}\;
  h_\gamma^{\text{hard}}(y) = I(|y|>\gamma)\, y
  $$

- **SCAD penalty:**  
  $$
  p^{\text{SCAD}}_\gamma(|\theta|) =
  \begin{cases}
    2\gamma|\theta|, & |\theta|\leq \gamma, \\
    \dfrac{\theta^2 - 2 a \gamma|\theta| + \gamma^2}{a-1}, & \gamma < |\theta|\leq a\gamma, \\
    \dfrac{(a+1)\gamma^2}{2}, & |\theta| > a\gamma,
  \end{cases}
  \;\xrightarrow[\text{minimizer}]{}\;
  h_\gamma^{\text{SCAD}}(y) =
  \begin{cases}
    \operatorname{sign}(y)(|y|-\gamma)_{+}, & |y| \leq 2\gamma, \\
    \dfrac{(a-1)y - \operatorname{sign}(y)a\gamma}{a-2}, & 2\gamma < |y|\leq a\gamma, \\
    y, & |y| > a\gamma,
  \end{cases}
  $$
\normalsize

where $a=3.7$ (\cite{fan2001}).
 



## sFPCA-rSVD Algorithm
To implement the sPCA-rSVD algorithm discussed above, we use the following iterative procedure to minimize the objective function defined in Equation \eqref{eq:sparse}. 

```{=latex}
\begin{block}{Algorithm: sFPCA-rSVD}
\begin{enumerate}
  \item {Initialization:}  
        Compute the best rank-one approximation of $X$ using singular value decomposition (SVD),  
        where $X \approx s u v^\top$, and set $u \leftarrow s u$.
  \item Iterate until convergence: 
    \begin{enumerate}
      \item {Update Left Singular Vector:} $u \leftarrow Xv$
      \item {Update Right Singular Vector:} $v \leftarrow \textcolor{TitleBlue}{h_\gamma} X^\top u$
      \item {Normalize Right Singular Vector:} $v \leftarrow \dfrac{v}{\lVert v \rVert}$
    \end{enumerate}
\end{enumerate}
\end{block}
```

## Cross-Validation for Sparsity Selection
- **Sparsity parameter**: Tuning parameter controlling number of non-zero loadings in $v$ ($0 =$ dense, $p =$ full sparsity).  

```{=latex}
\begin{block}{Algorithm: K-fold CV Tuning Parameter Selection - Degree of sparsity}
\label{algo:CvSparse}
\small
\begin{enumerate}
   \item Randomly group the rows of side-by-side data matrix $X$ into $K$ roughly equal-sized groups, denoted as $X^1,...,X^K$. 
   \item For each sparse tuning parameter $j \in \{0,1,...,p\}$ (level of sparsity), do the following: 
   \begin{enumerate}
     \item For $k = 1,...,K$, let $X^{-k}$ be the data matrix $X$ leaving out $X^k$. Apply Algorithm sFPCA-rSVD on $X^{-k}$ and derive the FPC scores $u^{-k}(j)$. Then project $X^k$ onto $u^{-k}(j)$ to obtain $v^k(j)$.
     \item Calculate the K-fold CV scores defined as: ($N$ is the number of grid points in $X^k$)
     \vspace{-1em}
     \[
     \textcolor{TitleBlue}{CV_j = \sum_{k=1}^{K} \frac{\Vert X^k - u^{-k}(j)v^{k}(j)\Vert^2}{N}}
     \]
     \vspace{-1em}
   \end{enumerate}
    \item Select the degree of sparsity as $j_0 = \arg\min\{CV(j)\}$.
\end{enumerate}
\end{block}
```


## Overview of Existing Approaches

- **\textcolor{TitleRed}{Smooth FPCA}:**  
  - Pros: Produces smooth eigenfunctions.  
  - Algorithm: Penalized power iteration.  
  - Tuning: $\color{TitleRed}\alpha$ (smoothness) via \textcolor{TitleRed}{GCV}.  

- **\textcolor{TitleBlue}{Sparse FPCA}:**  
  - Pros: Feature selection → interpretable.  
  - Algorithm: sFPCA-rSVD algorithm.  
  - Tuning: $\color{TitleBlue}\gamma$ (sparsity) via \textcolor{TitleBlue}{CV}.  

- **Combined Approaches:** Smooth + Sparse together.  
  $$
  \min_{u,v} \|X - uv^\top\|_F^2 + \color{TitleRed}{\alpha v^\top \Omega v} \color{black}+ \color{TitleBlue}{p_\gamma(v)}
  $$  
- Trade-off: **Variance explained vs Interpretability vs Smoothness**.  


# Regularized MFPCA {auto-animate="true"}
\hypertarget{sec:svd}{}
- **Context**  
  - Univariate FDA → **Multivariate FDA** (e.g., simultaneously recorded EEG channels, growth patterns of multiple anatomical measures.)  
  - MFPCA → **joint modes of variation** across functions  

- **Challenges**  
  - Discretization & irregular grids → noise, missing data  
  - High dimensionality and limited sample size → unstable eigenfunctions (sensitive to small fluctuations in the data)
  - Cross-function correlation → requires enforcing smoothness both within and across functions

- **Proposed Solution: Penalized SVD**  
  - **\textcolor{TitleRed}{Smoothness} penalties**: roughness on derivatives  
  - **\textcolor{TitleBlue}{Sparsity} penalties**: Soft, hard, or SCAD 
  - **Block-diagonal roughness matrix** for cross-function structure  

- **Impact**  
  - Produces **smooth, sparse, interpretable** joint modes  
  - More stable & applicable to high-dimensional multivariate FDA


## Methodology: Multivariate Functional Data Framework

- A multivariate functional dataset is formed by **concatenating $p$ functional data matrices**.  
  - Each variable: $X_i \in \mathbb{R}^{n \times m_i}$  where $n$: number of observations and $m_i$: grid points

::: incremental

- **Rank-one approximation** (per variable):  
  $$
  X_i \approx u_i v_i^\top, 
  \quad u_i \in \mathbb{R}^n, \; v_i \in \mathbb{R}^{m_i}
  $$

- **Full data matrix**:  $\mathbf{X} = [X_1\quad X_2\quad \cdots\quad X_p]  \in \mathbb{R}^{n \times \sum_{i=1}^p m_i}$
 
  $$
  \pmb{X}  = 
\begin{bmatrix}
x_{11}(t_{11}) & \cdots & x_{11}(t_{1,m_1}) & \cdots & x_{1p}(t_{p1}) & \cdots & x_{1p}(t_{p,m_p})\\
\vdots         & \ddots & \vdots             & \ddots & \vdots         & \ddots & \vdots\\
x_{n1}(t_{11}) & \cdots & x_{n1}(t_{1,m_1}) & \cdots & x_{np}(t_{p1}) & \cdots & x_{np}(t_{p,m_p})
\end{bmatrix}.
  $$
:::

## Penalized Smooth MFPCA {auto-animate="true"}


- Standard FPCA loadings may be noisy; smoothness penalties (via block-diagonal $\Omega_i$) improve structure and interpretability.  

::: incremental

- Let $\pmb{X} \in \mathbb{R}^{n \times M}$ denote multivariate functional data, where $M = \sum_{i=1}^p m_i$.  
  Its best rank-one approximation is $\pmb{X} \approx u v^\top$,  
  with $u \in \mathbb{R}^n$ (score vector) and $v = [v_1,v_2,..., v_p]^\top \in \mathbb{R}^M$ (loading vector).  
  A smoothness penalty is imposed on $v$.
  

- The block-diagonal penalty matrix is  $\color{TitleRed}{\pmb{\Omega} = \operatorname{diag}\bigl(\Omega_1, \Omega_2, \dots, \Omega_p\bigr)}$,
  where each $\Omega_i \in \mathbb{R}^{m_i \times m_i}$ is a univariate roughness penalty matrix.  

- The penalized reconstruction error is  
  $$
  \min_{u,v} \; \lVert \pmb{X} - uv^\top \rVert_F^2 
  + \color{TitleRed}{\pmb{\alpha}^\top \big(v^\top \pmb{\Omega} v\big)}\color{black}{,}
  $$
  where $\pmb{\alpha} = (\alpha_1, \dots, \alpha_p)^\top$ controls smoothness.  

:::


## MFPCA Power Algorithm
::: incremental
```{=latex}
        \begin{block}{Algorithm: Regularized Power Iteration for Smooth MFPCA}
\small
\begin{enumerate}
  \item Initialize $v$.
  \item Repeat until convergence:
    \begin{enumerate}
      \item $u \leftarrow \mathbf{X}v$
      \item $v \leftarrow \textcolor{TitleRed}{(I + \boldsymbol{\alpha}\boldsymbol{\Omega})^{-1}}\mathbf{X}^\top u$
      \item $v \leftarrow v / \lVert v \rVert$
    \end{enumerate}
  \item Update $\mathbf{X} \leftarrow \mathbf{X} - \sigma u v^\top$ to extract the next PC.
\end{enumerate}
\end{block}
```
- The smoothing operator is $\color{TitleRed}{\pmb{S}(\pmb{\alpha}) = \big(I + \pmb{\alpha}\pmb{\Omega}\big)^{-1}} \color{black}\in \mathbb{R}^{M \times M}$. 
- The smoothing parameter $\pmb{\alpha}$ is selected via generalized cross-validation (GCV), defined as
    ```{=latex}
  \vspace{-1.5em}
\begin{equation}\label{eq:MGCV one-way}
	\textcolor{TitleRed}{\text{GCV}(\pmb{\alpha}) = \frac{1}{M} \frac{\Vert ({I} - \pmb{S}(\pmb{\alpha})) (\mathbf{X}^T u) \Vert^2}{\left(1 - \frac{1}{M} \text{tr} \{\mathbf{S}(\pmb{\alpha})\}\right)^2}}.
\end{equation}
    ```
:::



## Penalized Sparse Multivariate FPCA

- **Goal:** Extend sparse FPCA to multivariate functional data, imposing **\textcolor{TitleBlue}{sparsity}** (select important regions) and **smoothness** (reduce noise).
- **Sparsity penalties:** Soft, hard, or SCAD thresholding \cite{huang2008sparse,sparseRegression2017,SparsePCA2020}.  
    ```{=latex}
    \pause
    ```
-  Sparsity parameters: $\pmb{\gamma}=(\gamma_1,\ldots,\gamma_p)$, where $\gamma_i$ ranges from 0 (no sparsity) to $m_i$ for each variable.
```{=latex}
    \begin{block}{Algorithm: Regularized Power Iteration for Smooth MFPCA}
      \small
      \begin{enumerate}
        \item {Initialization:}
              Compute rank-one SVD of $\pmb{X}$, $\pmb{X}\approx suv^\top$, and set $u \gets su$.
        \vspace{-0.5em}      
        \item {Iterate until convergence:}
          \begin{enumerate}
            \item {Update left singular vector:} $u \leftarrow \pmb{X}v$
            \item {Update right singular vector:} $v \leftarrow \textcolor{TitleBlue}{\pmb{h_\gamma}}\,\pmb{X}^\top u$
            \item {Normalize right singular vector:} $v \leftarrow \dfrac{v}{\lVert v\rVert}$
          \end{enumerate}
      \end{enumerate}
      \end{block}
```


## Smooth and Sparse Multivariate FPCA
- The combined implementation of smoothness and sparsity on the loading vector $v$ in multivariate functional data is achieved by the following algorithm:

```{=latex}
\begin{block}{Algorithm: Regularized Power Iteration for Smooth MFPCA}
\small
\begin{enumerate}
  \item Initialize unit vectors $u$ and $v$ using SVD of $\pmb{X}$ (best rank-one approximation of $\pmb{X}$) 
   \item Repeat till convergence 
   \begin{enumerate}
     \item ${u} \leftarrow\pmb{X}{v}$
     \item ${v} \leftarrow \textcolor{TitleRed}{\pmb{S}(\pmb{\alpha})} \textcolor{TitleBlue}{\pmb{h}(\gamma_v)} \pmb{X}^\top {u}$ 
     \item ${v}\leftarrow \frac{{v}}{\Vert {v}\Vert}$
   \end{enumerate}
   \item Update $\pmb{X} = \pmb{X} -\sigma uv^\top$ and proceed to find the next principal component. 
\end{enumerate}
\end{block}
```

- \textcolor{TitleBlue}{\hyperlink{algo:CvSparse}{Algorithm CV Tuning for Sparsity}} and \textcolor{TitleRed}{equation \eqref{eq:MGCV one-way}} are used to tune the sparsity level via K‐fold CV and the smoothing parameter via GCV, respectively.



## Simulation: Estimation Performance

- **Data-generating process:** Two functional variables:
  $$
  X^{(1)}_{ij} = u_{i1} v_{11}(t_j) + u_{i2} v_{12}(t_j) + \epsilon_{ij}^{(1)},
  \quad
  X^{(2)}_{ij} = u_{i1} v_{21}(t_j) + u_{i2} v_{22}(t_j) + \epsilon_{ij}^{(2)},
  $$
  - where $u_{i1}\sim N(0,\sigma_1^2),\; u_{i2}\sim N(0,\sigma_2^2)$, $\epsilon_{ij}^{(k)} \sim N(0,\sigma^2)$, and $n=m=101,\; t_j \in [-1,1]$  

- **True functional PCs:**  
  - Variable 1:  
  
  ```{=latex}
  \vspace{-10mm}
    $$
      v_{11}(t)=\tfrac{t+\sin(\pi t)}{s_1},\qquad
      v_{12}(t)=\tfrac{\cos(3\pi t)}{s_2}
    $$
    ```
  - Variable 2:  
  
  ```{=latex}
  \vspace{-8mm}
    $$
      v_{21}(t)=
      \begin{cases}
        \dfrac{\sin(3\pi t)}{s_3}, & t \in \bigl(-\tfrac13,\tfrac13\bigr),\\
        0, & \text{otherwise},
      \end{cases}
      \qquad
      v_{22}(t)=
      \begin{cases}
        \dfrac{\sin(2\pi t)}{s_4}, & t \le -\tfrac13,\\
        \dfrac{\sin(\pi t)}{s_4}, & t \ge \tfrac13,\\
        0, & \text{otherwise}.
      \end{cases}
    $$
    \smallskip
    Here, $s_1,s_2,s_3,s_4$ are normalizing constants ensuring unit $L^2$ norm.
    ```

## Simulation: Estimation Performance
**Scenarios tested:**
```{=latex}
\scriptsize 
1. Unpenalized Multivariate SVD (baseline)  \\
2. Smoothed Multivariate SVD (smoothness penalty)\\  
3. Sparse Multivariate SVD (sparsity penalty)  \\
4. Sparse + Smoothed Multivariate SVD (combined regularization)  
\vspace{-6mm}
\begin{figure}[!h]
  \centering
  \begin{minipage}[t]{0.25\textwidth}\centering
    \onslide<2->{%
      \includegraphics[width=\linewidth]{\detokenize{images/MFPCA-PC-Simulation1.pdf}}%
      }
  \end{minipage}\hfill
  \begin{minipage}[t]{0.25\textwidth}\centering
    \onslide<3->{%
      \includegraphics[width=\linewidth]{\detokenize{images/SmoothMFPCA-PC-Simulation1.pdf}}%
      }
  \end{minipage}\hfill
  \begin{minipage}[t]{0.25\textwidth}\centering
    \onslide<4->{%
      \includegraphics[width=\linewidth]{\detokenize{images/SparseMFPCA-PC-Simulation1.pdf}}%
      }
  \end{minipage}\hfill
  \begin{minipage}[t]{0.25\textwidth}\centering
    \onslide<5->{%
      \includegraphics[width=\linewidth]{\detokenize{images/SmSpMFPCA-PC-Simulation1.pdf}}%
      }
  \end{minipage}
\end{figure}
```


## Simulation: Estimation Performance

**Accuracy measures**:

1. Variable-wise MSE: 
   $$\mathrm{MSE}_{k\ell} = \tfrac{1}{m}\sum_{j=1}^m(\hat v_{k\ell}(t_j)-v_{k\ell}(t_j))^2$$
2. Replication-averaged MSE:
   $$\overline{\mathrm{MSE}}_{k\ell} = \tfrac{1}{R}\sum_{r=1}^R \mathrm{MSE}_{k\ell}^{(r)}$$  
3. Multivariate MSE:  
   $$\mathrm{MSE}_\ell^{(\mathrm{multi})} = \tfrac{1}{m}\sum_{j=1}^m\sum_{k=1}^2(\hat v_{k\ell}(t_j)-v_{k\ell}(t_j))^2$$


## Simulation: Estimation Performance

- **Performance across four methods (SVD, Smooth, Sparse, Smooth+Sparse):**
  - Smoothness and/or sparsity **reduce MSE** compared to unregularized SVD.  
  - **Smooth+Sparse** yields lowest error and most stable estimates.  
  - Smooth estimator performs consistently well; sparsity alone less effective (esp. for PC2).  
  - Joint regularization achieves best **bias–variance tradeoff**.  

```{=latex}
\begin{columns}[T,onlytextwidth]
  \begin{column}{0.55\textwidth}
    \centering
    \includegraphics[height=0.6\textheight]{\detokenize{images/MSE_methods_boxplot-Simulation1.pdf}}\\
  \end{column}
  
  \begin{column}{0.45\textwidth}
  \vspace*{2em}
    {\tiny
    \textbf{PC1: Quartiles and Mean log10(MSE)}\\[0.2em]
    \begin{tabular}{lrrrr}
    \hline
    Method & Q1 & Median & Mean & Q3\\ \hline
    SVD & -3.41 & -3.35 & -3.34 & -3.28\\
    Smooth & -4.07 & -3.96 & -3.92 & -3.82\\
    Sparse & -3.57 & -3.50 & -3.49 & -3.43\\
    Smooth+Sparse & \textbf{-4.38} & \textbf{-4.28} & \textbf{-4.22} & \textbf{-4.14}\\
    \hline
    \end{tabular}\\[0.6em]

    \textbf{PC2: Quartiles and Mean log10(MSE)}\\[0.2em]
    \begin{tabular}{lrrrr}
    \hline
    Method & Q1 & Median & Mean & Q3\\ \hline
    SVD & -2.84 & -2.79 & -2.79 & -2.75\\
    Smooth & -3.56 & -3.48 & -3.47 & -3.40\\
    Sparse & -2.83 & -2.79 & -2.79 & -2.75\\
    Smooth+Sparse & \textbf{-3.59} & \textbf{-3.50} & \textbf{-3.49} & \textbf{-3.42}\\
    \hline
    \end{tabular}
    }% end \scriptsize scope
  \end{column}
\end{columns}
```



## Application: Household Power Consumption

::: columns
::: {.column width="45%"}
- **Dataset:** Bivariate functional data including active power and voltage consumption [@HouseholdElectricPower] for one household between December 2006 and November 2010.
- **Scaling:** To equalize the contribution of each variable in the multivariate analysis, we rescale them following [@Happ_2018].
$\tilde{X}_j(t_i) = \hat{w}_j^{1/2} X_j(t_i),$
$\hat{w}_j = \left( \frac{1}{m} \sum_{i=1}^{m} \widehat{\mathrm{Var}}(X_j(t_i)) \right)^{-1}.$
:::

::: {.column width="55%"}
![](images/Electrical_PCS.pdf){height="0.55\textheight"}

\centering \tiny First 3 PCs: MFPCA (red) vs ReMFPCA (black)
:::
:::
Regularization reduces noise while preserving the dominant daily consumption patterns, enhancing interpretability without losing key structure.


# Two-way Regularized MFPCA {auto-animate="true"}
\hypertarget{sec:two_way}{}
- **Two-way functional data:** Each observation is a *matrix of curves*, with smooth variation across **two domains**  (e.g., time × space in air quality, time × channels in EEG).  
```{=latex}
\pause
```
- **Limitation of standard FPCA [@ramsay2005functional]:**  
  - Focuses on one domain (often time).  
  - Penalties applied only to loadings → ignores structure in second domain.  
  - Results may be rough or overly dense along the unpenalized axis.  

```{=latex}
\pause
```

- **Two-way FPCA [@twoway_huang]:**  
  - Introduced **smoothness penalties** on both scores and loadings.  
  - Produces coherent, interpretable *component surfaces* instead of jagged approximations.  

```{=latex}
\pause
```

- **Our contribution:**  
  - Extend to **multivariate functional data** (multiple functional variables).  
  - Combine **\textcolor{TitleRed}{smoothness} + \textcolor{TitleBlue}{sparsity} penalties** in both directions.  
  - Result: Low-rank, interpretable, noise-robust principal components for high-dimensional applications.  

  

## Two-way Smoothed MFPCA: Setup & Penalty

- Two-way multivariate functional data: $\pmb X \in \mathbb{R}^{n\times M}, \quad M=\sum_{i=1}^p m_i.$  

- Roughness matrices: $\pmb\Omega_u\in\mathbb{R}^{n\times n},\;\pmb\Omega_v\in\mathbb{R}^{M\times M}$ (symmetric, non-negative definite).  


- Smoothers: $\color{TitleRed}\pmb S_u(\alpha_u)=(I+\alpha_u\,\pmb\Omega_u)^{-1}, \qquad \pmb S_v(\pmb\alpha_v)=(I+\pmb\alpha_v\,\pmb\Omega_v)^{-1}.$

```{=latex}
\pause
```

- Penalized rank-one reconstruction:  
  $$
  \min_{u,v}\;\|\pmb X-uv^\top\|_F^2+ \color{TitleRed} \mathcal P(u,v)
  $$

- Penalty [@twoway_huang]:  $\mathcal P(u,v;\alpha_u,\pmb\alpha_v) = u^\top(\alpha_u\pmb\Omega_u)u\,\|v\|^2 + \|u\|^2\,v^\top(\pmb\alpha_v\pmb\Omega_v)v + u^\top(\alpha_u\pmb\Omega_u)u\; v^\top(\pmb\alpha_v\pmb\Omega_v)v.$

- Multivariate $v$:  $\pmb\Omega_v=\operatorname{diag}(\Omega_1,\ldots,\Omega_p).$


## Two-way Smoothed MFPCA: Conditional GCV

- Minimizers:  
    ```{=latex}
    \vspace{-3mm}
    \small
\[
u=\frac{\pmb S_u(\alpha_u)\,\pmb X v}{v^\top (I+\pmb{\alpha}_v\,\pmb{\Omega}_v)\,v}
=\frac{\pmb S_u(\alpha_u)}{1+\pmb{\alpha}_v\,\pmb R_v(v)}\;\frac{\pmb X v}{\|v\|^2},
\qquad
v=\frac{\pmb S_v(\pmb{\alpha}_v)\,\pmb X^\top u}{u^\top (I+\alpha_u\,\pmb{\Omega}_u)\,u}
=\frac{\pmb S_v(\pmb{\alpha}_v)}{1+\alpha_u\,\pmb R_u(u)}\;\frac{\pmb X^\top u}{\|u\|^2}.
\]
    ```

- Rayleigh quotients:  $\pmb R_u(u)=\frac{u^\top\pmb\Omega_u u}{\|u\|^2}, \qquad \pmb R_v(v)=\frac{v^\top\pmb\Omega_v v}{\|v\|^2}.$

```{=latex}
\pause
```

- Conditional GCV criteria:  
    ```{=latex}
    \vspace{-4mm}
  \small
\[
\mathrm{GCV}_u(\alpha_u;\pmb{\alpha}_v)=
\frac{\tfrac1n\left\|\Big(I-\tfrac{\pmb S_u(\alpha_u)}{1+\pmb{\alpha}_v\,\pmb R_v(v)}\Big)\tfrac{\pmb X v}{\|v\|^2}\right\|^2}
{\Big(1-\tfrac1n\,\mathrm{tr}\!\big(\tfrac{\pmb S_u(\alpha_u)}{1+\pmb{\alpha}_v\,\pmb R_v(v)}\big)\Big)^2},
\qquad
\mathrm{GCV}_v(\pmb{\alpha}_v;\alpha_u)=
\frac{\tfrac1m\left\|\Big(I-\tfrac{\pmb S_v(\pmb{\alpha}_v)}{1+\alpha_u\,\pmb R_u(u)}\Big)\tfrac{\pmb X^\top u}{\|u\|^2}\right\|^2}
{\Big(1-\tfrac1m\,\mathrm{tr}\!\big(\tfrac{\pmb S_v(\pmb{\alpha}_v)}{1+\alpha_u\,\pmb R_u(u)}\big)\Big)^2}.
\]
    ```

- **Optimization:** Alternate updates of $u$ and $v$ using GCV until convergence → two-way regularized components.  


## Two-way Smooth + Sparse MFPCA

- **Goal:** Extract components that are **low-rank, smooth, and sparse**.  
  - \textcolor{TitleRed}{Smoothness} → coherent variation across subjects & functions.  
  - \textcolor{TitleBlue}{Sparsity} → highlights key observations & time regions.  

- **Novelty:** First framework to combine **both** in two-way functional data.  

```{=latex}
\pause
```

- Data matrix $\pmb{X}$: seek $u, v$ solving:
  $$
  \min_{u,v} \|\pmb X - uv^\top\|_F^2 + 
  \sum_j^J \mathcal P_j^{[\theta]}(u,v)
  $$
- $J$ is the number of penalty components, and $\theta$ is the vector of all tuning parameters.
- The composite penalty $\sum_{j=1}^J \mathcal{P}_j^{(\theta)}(u,v)$ lets us mix regularizers, e.g., \textcolor{TitleRed}{smoothness} with $\color{TitleRed}{\theta=(\alpha_u,\boldsymbol{\alpha}_v)}$ and \textcolor{TitleBlue}{sparsity} with $\textcolor{TitleBlue}{\theta=(\gamma_u,\boldsymbol{\gamma}_v)}$ (controlling sparsity), and can include other structures as needed.
 

## Sequential Power Algorithm

```{=latex}
\begin{block}{Algorithm: Two-way Smooth + Sparse MFPCA (Sequential Power)}\label{algo:twoway-smsp}
\label{algo:SSPower}
\small
\begin{enumerate}
  \item {Initialization:} Rank-one SVD of $\pmb X$: $\pmb X \approx s\,u^{(0)} {v^{(0)}}^\top$; set $u \leftarrow s\,u^{(0)}$, $v \leftarrow v^{(0)}$.
  \item {Repeat until convergence:}
    \begin{enumerate}
      \item $u \leftarrow \textcolor{TitleRed}{\pmb S_u^{[\alpha_u]}}\; \textcolor{TitleBlue}{\pmb h_u^{[\gamma_u]}} (\pmb X\,v)$
      \item $v \leftarrow \textcolor{TitleRed}{\pmb S_v^{[\pmb{\alpha}_v]}}\; \textcolor{TitleBlue}{\pmb h_v^{[\pmb{\gamma}_v]}}(\pmb X^\top u)$
      \item $v \leftarrow v/\lVert v\rVert$
    \end{enumerate}
  \item $\pmb X \leftarrow \pmb X - \sigma\,u\,v^\top$ to extract the next component.
\end{enumerate}
\end{block}
```

- Smoothness parameters are selected with conditional GCV, while sparsity parameters are chosen via cross-validation (CV).


## Selection of Regularization Parameters

- Four sets of tuning parameters:  
  - $\color{TitleRed}{\alpha_u}$: smoothness of $u$,  $\color{TitleBlue}\gamma_u$: sparsity of $u$  
  - $\color{TitleRed}{\pmb{\alpha}_v}$: smoothness of $v$,  $\color{TitleBlue}\pmb{\gamma}_v$: sparsity of $v$  

- **Challenge:** Ordering of tuning (smoothness vs. sparsity) affects convergence and solutions.  

```{=latex}
\pause
```

- **Strategy: Conditional tuning**  
  1. Initialize all penalties at 0.  
  2. Tune $\gamma_u$ via $K$-fold CV.  
  3. Sequentially tune $\gamma_{v,i}$ using \hyperlink{algo:SSPower}{Algorithm:Two-way Smooth + Sparse MFPCA}.
  4. With sparsity fixed, tune $\alpha_u$ by GCV.  
  5. Tune $\alpha_{v,i}$ using two-way GCV.  
  6. Iterate steps 2–5 until stable.  

- This alternating scheme **isolates sparsity vs. smoothness** while ensuring accuracy + interpretability.  

## K-Fold CV algorithm for Sparsity
```{=latex}
\footnotesize
\begin{columns}[T,onlytextwidth]

% ===================== Left: Plain K-fold CV =====================
\begin{column}{0.48\textwidth}
\begin{block}{K-Fold CV (\textcolor{Clay}{Row} Sparsity)}
\begin{enumerate}
  \item Split $\pmb X\in\mathbb R^{n\times M}$ into $K$ \textcolor{Clay}{column groups} 
        $\{\pmb X^{(1)},\dots,\pmb X^{(K)}\}$.
  \item For each $\gamma_j$ and $k=1,\dots,K$:
    \begin{enumerate}
      \item Train on $\pmb X^{(-k)}$, estimate $\textcolor{Clay}{u_j^{(-k)}}$.
      \item Validate: $\textcolor{Clay}{v_j^{(k)}=\pmb X^{(k)\top}u_j^{(-k)}}$.
      \item Fold error:
        \[
          \mathrm{Err}^{(k)}_j=\tfrac{1}{\tilde M}\,
          \|\pmb X^{(k)}-\textcolor{Clay}{u_j^{(-k)}(v_j^{(k)})^\top}\|_F^2.
        \]
    \end{enumerate}
  \item CV score: $\widehat{\mathrm{CV}}_j=\tfrac{1}{K}\sum_k \mathrm{Err}^{(k)}_j$.
  \item Select $j_0=\arg\min_j \widehat{\mathrm{CV}}_j$.
\end{enumerate}
\end{block}
\end{column}

% ===================== Right: 1-SE Rule =====================
\begin{column}{0.48\textwidth}
\begin{block}{K-Fold CV + 1-SE Rule}
\begin{enumerate}
  \item Use same folds to collect $\mathrm{Err}^{(k)}_j$.
  \item Compute mean $\widehat{\mathrm{CV}}_j$ and SE $\widehat{\mathrm{SE}}_j$:
    \[
    \widehat{\mathrm{SE}}_j=\sqrt{\tfrac{1}{K(K-1)}\sum_k
    (\mathrm{Err}^{(k)}_j-\widehat{\mathrm{CV}}_j)^2}.
    \]
  \item Let $j^\star=\arg\min_j \widehat{\mathrm{CV}}_j$.
  \item Choose sparsest $j_0$ with
    $\widehat{\mathrm{CV}}_j \le
     \widehat{\mathrm{CV}}_{j^\star}+\widehat{\mathrm{SE}}_{j^\star}$.
\end{enumerate}
\end{block}
\end{column}

\end{columns}
```

## K-Fold CV algorithm for Sparsity
```{=latex}
\footnotesize
\begin{columns}[T,onlytextwidth]

% ===================== Left: Plain K-fold CV =====================
\begin{column}{0.48\textwidth}
\begin{block}{K-Fold CV (\textcolor{DarkGreen}{Column} Sparsity)}
\begin{enumerate}
  \item Split $\pmb X\in\mathbb R^{n\times M}$ into $K$ \textcolor{DarkGreen}{row groups} 
        $\{\pmb X^{(1)},\dots,\pmb X^{(K)}\}$.
  \item For each $\gamma_j$ and $k=1,\dots,K$:
    \begin{enumerate}
      \item Train on $\pmb X^{(-k)}$, estimate $\textcolor{DarkGreen}{v_j^{(-k)}}$.
      \item Validate: $\textcolor{DarkGreen}{u_j^{(k)}=\pmb X^{(k)} v_j^{(-k)}}$.
      \item Fold error:
        \[
          \mathrm{Err}^{(k)}_j=\tfrac{1}{\tilde n}\,
          \|\pmb X^{(k)}-\textcolor{DarkGreen}{u_j^{(k)}(v_j^{(-k)})^\top}\|_F^2.
        \]
    \end{enumerate}
  \item CV score: $\widehat{\mathrm{CV}}_j=\tfrac{1}{K}\sum_k \mathrm{Err}^{(k)}_j$.
  \item Select $j_0=\arg\min_j \widehat{\mathrm{CV}}_j$.
\end{enumerate}
\end{block}
\end{column}

% ===================== Right: 1-SE Rule =====================
\begin{column}{0.48\textwidth}
\begin{block}{K-Fold CV + 1-SE Rule}
\begin{enumerate}
  \item Use same folds to collect $\mathrm{Err}^{(k)}_j$.
  \item Compute mean $\widehat{\mathrm{CV}}_j$ and SE $\widehat{\mathrm{SE}}_j$:
    \[
    \widehat{\mathrm{SE}}_j=\sqrt{\tfrac{1}{K(K-1)}\sum_k
    (\mathrm{Err}^{(k)}_j-\widehat{\mathrm{CV}}_j)^2}.
    \]
  \item Let $j^\star=\arg\min_j \widehat{\mathrm{CV}}_j$.
  \item Choose sparsest $j_0$ with
    $\widehat{\mathrm{CV}}_j \le
     \widehat{\mathrm{CV}}_{j^\star}+\widehat{\mathrm{SE}}_{j^\star}$.
\end{enumerate}
\end{block}
\end{column}

\end{columns}
```


## Chessboard
```{=latex}
\begin{figure}
\centering
\setlength{\tabcolsep}{6pt}
\begin{tabular}{cccc}
% ---------- Row 1 ----------
\begin{minipage}[c]{0.23\textwidth}\centering
  \uncover<1->{\includegraphics[width=.8\linewidth]{images/True.pdf}}
\end{minipage} &
\begin{minipage}[c]{0.23\textwidth}\centering
  \uncover<3->{\includegraphics[width=.8\linewidth]{images/svd_Reconstructed.pdf}}
\end{minipage} &
\begin{minipage}[c]{0.23\textwidth}\centering
  \uncover<5->{\includegraphics[width=.8\linewidth]{images/Lasso_reconstructed.pdf}}
\end{minipage} &
\begin{minipage}[c]{0.23\textwidth}\centering
  \begin{overlayarea}{\linewidth}{.9\linewidth}
    \only<3>{\includegraphics[width=.8\linewidth]{images/u1plots_SVD.jpg}}%
    \only<4>{\includegraphics[width=.8\linewidth]{images/u1plots_SVD_Smooth.png}}%
    \only<5>{\includegraphics[width=.8\linewidth]{images/u1plots_SVD_Smooth_Sparse.png}}%
    \only<6>{\includegraphics[width=.8\linewidth]{images/u1plots.pdf}}%
  \end{overlayarea}
\end{minipage} \\
% ---------- Row 2 ----------
\begin{minipage}[c]{0.23\textwidth}\centering
  \uncover<2->{\includegraphics[width=.8\linewidth]{images/NoisyChessboard.pdf}}
\end{minipage} &
\begin{minipage}[c]{0.23\textwidth}\centering
  \uncover<4->{\includegraphics[width=.8\linewidth]{images/smooth_reconstructed.pdf}}
\end{minipage} &
\begin{minipage}[c]{0.23\textwidth}\centering
  \uncover<6->{\includegraphics[width=.8\linewidth]{images/pic.compress.pdf}}
\end{minipage} &
\begin{minipage}[c]{0.23\textwidth}\centering
  \begin{overlayarea}{\linewidth}{.9\linewidth}
    \only<3>{\includegraphics[width=.8\linewidth]{images/v1plots_SVD.jpg}}%
    \only<4>{\includegraphics[width=.8\linewidth]{images/v1plots_SVD_Smooth.png}}%
    \only<5>{\includegraphics[width=.8\linewidth]{images/v1plots_SVD_Smooth_Sparse.png}}%
    \only<6>{\includegraphics[width=.8\linewidth]{images/v1plots.pdf}}%
  \end{overlayarea}
\end{minipage}
\end{tabular}
\end{figure}
```




## Variance Explained: Classical vs Regularized FPCA

::: incremental
- **Classical FPCA:** Loadings $v_j$ orthonormal; scores $u_j=Xv_j$ uncorrelated.  
  Variance explained by first $J$ PCs:
  $$
  \sum_{j=1}^J \|u_j\|^2
  = \mathrm{trace}\!\big(V_J^\top X^\top X\,V_J\big),\qquad
  V_J=[v_1,\dots,v_J].
  $$

- **Issue under regularization:** smoothness/sparsity break orthogonality → scores become correlated → naive sum $\sum \|u_j\|^2$ **double-counts** variance (cf. Huang et al., 2008).
:::


## Subspace-Projection Definition of Explained Variance

::: incremental
- Normalize loadings and stack:
  $$
  V_J=[v_1,\dots,v_J],\qquad v_j \leftarrow \frac{v_j}{\|v_j\|}.
  $$

- Orthogonal projector onto span $(v_1,\dots,v_J)$:
  $$
  H_J=V_J\,(V_J^\top V_J)^{-1} V_J^\top,\qquad
  $$
  where $H_{J}$ is a a symmetric idempotent matrix. ($H_{J}^{2}=H_{J},\; H_{J}^{\top}=H_{J}$)

- Projected data and explained variance:
  $$
  X_J=X H_J,\qquad
  \mathsf{V}_{\text{tot}}=\mathrm{tr}(X^\top X),\qquad
  \mathcal{V}_J=\|X_J\|_F^2=\mathrm{tr}\!\big(H_J X^\top X H_J\big).
  $$
:::


## PVE, Incremental PVE, and Properties

::: incremental
- Incremental variance:
  $$
  \Delta\mathcal{V}_j=\mathcal{V}_j-\mathcal{V}_{j-1},\qquad \mathcal{V}_0=0.
  $$

- **Proportion of variance explained (PVE):**
  $$
  \mathrm{PVE}(J)=\frac{\mathcal{V}_J}{\mathsf{V}_{\text{tot}}},\qquad
  \mathrm{pve}_j=\frac{\Delta\mathcal{V}_j}{\mathsf{V}_{\text{tot}}}
  =\mathrm{PVE}(j)-\mathrm{PVE}(j-1).
  $$

- **Key properties:**  
  • No double-counting (works with correlated scores).  
  • Reduces to classical PCA when $V_J^\top V_J=I_J$.  
  • Monotone in $J$ ($\mathcal{V}_J$ increases).  
  • $\Delta\mathcal{V}_j$ = unique variance added by component $j$.
:::

## Simulation: Two-way Functional Data 
- **Data-generating process:** $X^{(1)}_{ij} = u_{i1} v_{11}(t_j) + u_{i2} v_{12}(t_j) + \epsilon_{ij}^{(1)}, \quad X^{(2)}_{ij} = u_{i1} v_{21}(t_j) + u_{i2} v_{22}(t_j) + \epsilon_{ij}^{(2)},$
- **Latent scores:** generated as smooth functions  
    ```{=latex}
    \vspace{-4mm}
  $$
  u_{1}(s)=\begin{cases}
  \sin(\pi s), & s>0, \\ 
  0, & \text{otherwise},
  \end{cases}
  \quad
  u_{2}(s)=\sin(2\pi s),\quad s\in[-1,1].
  $$
    ```

- **Functional PCs:**  
  - Variable 1: $v_{11}(t)=\tfrac{t+\sin(\pi t)}{s_1},\qquad v_{12}(t)=\tfrac{\cos(3\pi t)}{s_2}$
  - Variable 2:  
  
    ```{=latex}
  \vspace{-8mm}
    $$
      v_{21}(t)=
      \begin{cases}
        \dfrac{\sin(3\pi t)}{s_3}, & t \in \bigl(-\tfrac13,\tfrac13\bigr),\\
        0, & \text{otherwise},
      \end{cases}
      \qquad
      v_{22}(t)=
      \begin{cases}
        \dfrac{\sin(2\pi t)}{s_4}, & t \le -\tfrac13,\\
        \dfrac{\sin(\pi t)}{s_4}, & t \ge \tfrac13,\\
        0, & \text{otherwise}.
      \end{cases}
    $$
    ```
  
  
  
## Evaluation Metrics 
- **Integrated Squared Error (ISE):**  
  For replicate $r$ and component $u_1$:  
  $$
  \mathrm{ISE}_r^{(u_1,\text{method})}
  = \frac{1}{m}\sum_{j=1}^m
  \Big(u_1(t_j) - \widehat u_1^{(\text{method})}(t_j)\Big)^2.
  $$
- **Relative ISE (R\_ISE):** ratio vs best method  
  $$
  R_r^{(u_1,\text{method})}
  = \frac{\mathrm{ISE}_r^{(u_1,\text{method})}}
         {\mathrm{ISE}_r^{(u_1,\text{best})}}.
  $$
- **Monte Carlo averages:**  
  $$
  \overline{R}^{(u_1,\text{method})}
  = \frac{1}{N}\sum_{r=1}^N R_r^{(u_1,\text{method})}, 
  \qquad
  \mathrm{SE}\big(\overline{R}\big)
  = \sqrt{\frac{1}{N(N-1)}\sum_{r=1}^N
  \Big(R_r - \overline{R}\Big)^2}.
  $$

## \SimResultsTitle

```{=latex}
\centering
\begin{minipage}{0.9\linewidth}
  \centering
  \only<1>{\includegraphics[width=\linewidth]{images/Simulation 2 - SVD.pdf} }%
  \only<2>{\includegraphics[width=\linewidth]{images/Simulation 2 - Smoothness_and_Sparsity_on_v.pdf} }%
  \only<3>{\includegraphics[width=\linewidth]{images/Simulation 2 - Smoothness_and_Sparsity_on_u.pdf} }%
  \only<4>{\includegraphics[width=\linewidth]{images/Simulation 2 - Two-Way_Smoothness.pdf}}%
  \only<5>{\includegraphics[width=\linewidth]{images/Simulation 2 - Two-Way_Sparsity.pdf}}%
  \only<6>{\includegraphics[width=\linewidth]{images/Simulation 2 - Two-way_Sparsity_and_Smoothness.pdf}}%
\end{minipage}
```


## Simulation Results
```{=latex}
\begin{columns}[T,onlytextwidth]

% -------- Left table: ISE --------
\begin{column}{0.51\textwidth}
\scriptsize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.05}

\begin{block}{Table 3: Mean ISE for each method and parameter}
\begin{tabular}{lrrrr}
\toprule
Method              & u1     & u2     & v1       & v2       \\
\midrule
SVD                 & 0.3651 & 0.1587 & 0.00005  & 0.00010  \\
Smooth+Sparse $v$   & 0.3651 & 0.1587 & \textbf{0.00002} & \textbf{0.00002} \\
Smooth+Sparse $u$   & \textbf{0.3650} & \textbf{0.1584} & 0.00005  & 0.00010  \\
Two-way Smoothness  & \textbf{0.3650} & 0.1585 & \textbf{0.00002} & \textbf{0.00002} \\
Two-way Sparsity    & 0.3651 & 0.1586 & 0.00004  & 0.00009  \\
\textbf{Two-way Sm+Sp} & \textbf{0.3650} & \textbf{0.1584} & \textbf{0.00002} & \textbf{0.00002} \\
\bottomrule
\end{tabular}
\end{block}

\end{column}

% -------- Right table: Relative ISE --------
\begin{column}{0.43\textwidth}
\scriptsize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.05}

\begin{block}{Table 4: Mean Relative ISE for each method and parameter}
\begin{tabular}{lrrrr}
\toprule
Method              & u1   & u2   & v1    & v2    \\
\midrule
SVD                 & 1.000 & 1.001 & 8.21  & 5.23 \\
Two-way Sparsity    & 1.000 & 1.001 & 7.71  & 4.61 \\
Smooth+Sparse $v$   & 1.000 & 1.001 & 1.05  & \textbf{1.01} \\
Smooth+Sparse $u$   & \textbf{1.000} & \textbf{1.000} & 8.19  & 5.21 \\
Two-way Smoothness  & \textbf{1.000} & \textbf{1.000} & \textbf{1.00} & 1.21 \\
\bottomrule
\end{tabular}
\end{block}

\end{column}

\end{columns}
```

- Two-way Smooth+Sparse consistently yields the lowest errors across $u$ and $v$.

## Application: Motion Sense Data

- **Dataset:** Acceleration and pitch from 24 people, 4 activities (jogging, walking, sitting, standing), about 2–3 min each.
- **Goal:** Compare SVD vs **two-way sparse + smooth ReMFPCA** on these multivariate functional signals.
- **Rescaling (Happ & Greven, 2018):** balance variables so each contributes equally.
  
$$
\hat{w}_j=\left(\frac{1}{m}\sum_{i=1}^m \widehat{\mathrm{Var}}\!\big(X_j(t_i)\big)\right)^{-1},
\qquad
\tilde{X}_j(t_i)=\hat{w}_j^{1/2}X_j(t_i).
$$

- **Penalties used:** smoothness + sparsity on loadings $v$; sparsity on scores $u$ (treated as random effects).

## Results: Functional PCs (SVD vs ReMFPCA)

::: columns
::: {.column width="100%"}
![](images/Motion Sense PCS.pdf){width="75%" fig-align="center"}
:::
:::

- **SVD (gray):** noisy, high-frequency wiggles.
- **ReMFPCA (black):** smoother, more interpretable PCs capturing dominant structure.

## Results: PC Scores and Interpretation

::: columns
::: {.column width="100%"}
![](images/Motion Sense PC Scores.pdf){width="70%" fig-align="center"}
:::
:::

- Sparsity on scores: **PC1 scores for walking about 0** (partially standing too).
- Interpretation: walking contributes little to PC1; removing it **improves interpretability** without hurting fit.
- Takeaway: **Two-way smooth + sparse ReMFPCA** yields cleaner PCs and activity-informative scores.


# Conclusion & Future Work {auto-animate="true"}
\hypertarget{sec:conclusion}{}

- **Unified FPCA Framework**  
  - Combines **smoothness** (denoise, interpretability) + **sparsity** (variable selection).  
  - Extends from **univariate → multivariate → two-way functional data**.  

::: {.fragment style="height: 0.5em"}
:::

- **Methodology**  
  - Penalized SVD with roughness + $\ell_1$ penalties.  
  - Two-way regularization: smoothness & sparsity on both **scores ($u$)** and **loadings ($v$)**.  
  - Efficient parameter tuning: **conditional GCV** & **K-fold CV** (with 1-SE rule).  

::: {.fragment style="height: 0.5em"}
:::

- **Results**  
  - Simulations & applications (mortality, call-center, image data).  
  - Outperforms one-way or single-penalty methods.  
  - Produces **low-rank, denoised, interpretable components**.  
  
  
## Accessible Implementation: R Package & Future Work

- Implemented in **R package `ReMPCA` ([GitHub](https://github.com/mobinapourmoshir/ReMPCA/tree/master))** 
  - Univariate & multivariate FPCA with penalties.  
  - Two-way MFPCA for matrix-valued functions.  
  - Automated tuning (CV, GCV, 1-SE rule).  
  - Diagnostic tools: variance explained, visualization, heatmaps.  
  - Early support for **hybrid data (scalar + functional + image)**.  
- **Hybrid Data Extensions**  
  - Image–Functional Hybrid PCA → simultaneous dimension reduction.  
  - Scalar–Functional Integration → joint low-dim space.  
  - Nonlinear Extensions → kernel FPCA, neural nets.  
- **Applications:**  
  - Neuroimaging  
  - Personalized medicine  
  - Environmental monitoring  
**Takeaway:**  Smooth + sparse + two-way FPCA offers a **theoretical foundation, practical algorithms, and open software** to enable next-generation functional data analysis.

## References 
```{=latex}
\tiny{
\bibliography{ref}
}
```

## Acknowledgments 
```{=latex}
\only<1>{\parbox{\textwidth}{This thesis would not have been possible without the support, guidance, and encouragement of many people.}}%

\only<2>{\parbox{\textwidth}{\textbf{Dr.\ Mehdi Maadooliat: }\\ First and foremost, I am deeply thankful to my advisor, Dr. Mehdi Maadooliat. His mentorship has shaped not only this work but also my outlook as a researcher. From the start of my graduate studies, he has been a constant source of patience, thoughtful feedback, and encouragement. His dedication and belief in my potential have been invaluable in my growth as a student.}}%

\only<3>{\parbox{\textwidth}{\textbf{Dr.\ Hossein Haghbin}\\ I would also like to acknowledge Dr. Hossein Haghbin, whose early guidance played an important role in directing my research interests. His insights and encouragement helped me approach functional data analysis with clarity and confidence.}}%

\only<4>{\parbox{\textwidth}{\textbf{Committee Members:}\\ My sincere appreciation extends to my committee members, Dr. Rebecca Sanders and Dr. Hossein Haghbin, for generously offering their time and expertise. Their feedback and perspectives have strengthened this thesis and enriched my learning.}}%

\only<5>{\parbox{\textwidth}{\textbf{Department and peers:}\\ I appreciate the Department of Mathematical and Statistical Sciences at Marquette University for a supportive, collaborative environment, and I thank my friends and labmates for their companionship, encouragement, and good humor.}}%

\only<6>{\parbox{\textwidth}{\textbf{My dear family:}\\ Most importantly, I am profoundly indebted to my family. To my parents and brothers, whose love and sacrifices have given me every opportunity to pursue my education. Thank you for your encouragement, for reminding me to keep perspective, and for always believing in me even when I doubted myself. \\ And to my husband, Pouya, words cannot fully capture my gratitude. His patience, unwavering support, and countless sacrifices have carried me through every stage of this journey. He has been my anchor during challenges and my greatest source of joy in moments of success. This achievement belongs as much to my family as it does to me.}}%
```

##  {.center .middle}

```{=latex}
\centering
\Huge 
\textcolor{TitleBlue}{Thank you!}

\vspace{8mm}

\centering\includegraphics[height=0.25\textheight]{images/MU.png}\par
```
