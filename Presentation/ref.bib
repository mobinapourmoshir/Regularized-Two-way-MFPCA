@article{huang2008functional,
author = {Jianhua Z. Huang and Haipeng Shen and Andreas Buja},
title = {{Functional principal components analysis via penalized rank one approximation}},
volume = {2},
journal = {Electronic Journal of Statistics},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {678 -- 695},
keywords = {Functional data analysis, Penalization, regularization, Singular value decomposition},
year = {2008},
doi = {10.1214/08-EJS218},
}



@article{huang2008sparse,
title = {Sparse principal component analysis via regularized low rank matrix approximation},
journal = {Journal of Multivariate Analysis},
volume = {99},
number = {6},
pages = {1015-1034},
year = {2008},
issn = {0047-259X},
author = {Haipeng Shen and Jianhua Z. Huang},
keywords = {Dimension reduction, High-dimension-low-sample-size, Regularization, Singular value decomposition, Thresholding},
abstract = {Principal component analysis (PCA) is a widely used tool for data analysis and dimension reduction in applications throughout science and engineering. However, the principal components (PCs) can sometimes be difficult to interpret, because they are linear combinations of all the original variables. To facilitate interpretation, sparse PCA produces modified PCs with sparse loadings, i.e. loadings with very few non-zero elements. In this paper, we propose a new sparse PCA method, namely sparse PCA via regularized SVD (sPCA-rSVD). We use the connection of PCA with singular value decomposition (SVD) of the data matrix and extract the PCs through solving a low rank matrix approximation problem. Regularization penalties are introduced to the corresponding minimization problem to promote sparsity in PC loadings. An efficient iterative algorithm is proposed for computation. Two tuning parameter selection methods are discussed. Some theoretical results are established to justify the use of sPCA-rSVD when only the data covariance matrix is available. In addition, we give a modified definition of variance explained by the sparse PCs. The sPCA-rSVD provides a uniform treatment of both classical multivariate data and high-dimension-low-sample-size (HDLSS) data. Further understanding of sPCA-rSVD and some existing alternatives is gained through simulation studies and real data examples, which suggests that sPCA-rSVD provides competitive results.}
}


@article{cookbook2008matrix,
  title={The matrix cookbook},
  author={Petersen, Kaare Brandt and Pedersen, Michael Syskind and others},
  journal={Technical University of Denmark},
  volume={7},
  number={15},
  pages={510},
  year={2008}
}


@article{fan2001,
author = {Jianqing Fan and Runze Li},
title = {Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties},
journal = {Journal of the American Statistical Association},
volume = {96},
number = {456},
pages = {1348-1360},
year = {2001},
publisher = {Taylor & Francis},
,
eprint = {

        https://doi.org/10.1198/016214501753382273



}

}



@book{ramsay2005functional,
  title={Functional Data Analysis},
  author={Ramsay, J. and Silverman, B.W.},
  isbn={9780387400808},
  lccn={2005923773},
  series={Springer Series in Statistics},
  year={2005},
  publisher={Springer}
}





@article{sparseRegression2017,
author = {Zhenhua Lin, Jiguo Cao, Liangliang Wang and Haonan Wang},
title = {Locally Sparse Estimator for Functional Linear Regression Models},
journal = {Journal of Computational and Graphical Statistics},
volume = {26},
number = {2},
pages = {306--318},
year = {2017},
publisher = {Taylor \& Francis},
eprint = {

        https://doi.org/10.1080/10618600.2016.1195273



}

}





@article{SparsePCA2020,
title = {Sparse functional principal component analysis in a new regression framework},
journal = {Computational Statistics \& Data Analysis},
volume = {152},
pages = {107016},
year = {2020},
issn = {0167-9473},
author = {Yunlong Nie and Jiguo Cao},
keywords = {Dimension reduction, Eigendecomposition, Empirical basis approximation, Functional data analysis},
abstract = {The functional principal component analysis is widely used to explore major sources of variation in a sample of random curves. These major sources of variation are represented by functional principal components (FPCs). The FPCs from the conventional FPCA method are often nonzero in the whole domain, and are hard to interpret in practice. The main focus is to estimate functional principal components (FPCs), which are only nonzero in subregions and are referred to as sparse FPCs. These sparse FPCs not only represent the major variation sources but also can be used to identify the subregions where those major variations exist. The current methods obtain sparse FPCs by adding a penalty term on the length of nonzero regions of FPCs in the conventional eigendecomposition framework. However, these methods become an NP-hard optimization problem. To overcome this issue, a novel regression framework is proposed to estimate FPCs and the corresponding optimization is not NP-hard. The FPCs estimated using the proposed sparse FPCA method is shown to be equivalent to the FPCs using the conventional FPCA method when the sparsity parameter is zero. Simulation studies illustrate that the proposed sparse FPCA method can provide more accurate estimates for FPCs than other available methods when those FPCs are only nonzero in subregions. The proposed method is demonstrated by exploring the major variations among the acceleration rate curves of 107 diesel trucks, where the nonzero regions of the estimated sparse FPCs are found well separated.}
}


@article{AIC,
  title={AIC, BIC and recent advances in model selection},
  author={Chakrabarti, Arijit and Ghosh, Jayanta K},
  journal={Philosophy of statistics},
  pages={583--605},
  year={2011},
  publisher={Elsevier}
}



@book{green1993nonparametric,
  title={Nonparametric regression and generalized linear models: a roughness penalty approach},
  author={Green, Peter J and Silverman, Bernard W},
  year={1993},
  publisher={Crc Press}
}


@article{LassoTibshirani,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {Regression Shrinkage and Selection via the Lasso},
 urldate = {2024-09-09},
 volume = {58},
 year = {1996}
}


@article{grouplassoYuan,
    author = {Yuan, Ming and Lin, Yi},
    title = "{Model Selection and Estimation in Regression with Grouped Variables}",
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {68},
    number = {1},
    pages = {49-67},
    year = {2005},
    month = {12},
    abstract = "{We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.}",
    issn = {1369-7412},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/68/1/49/49794691/jrsssb\_68\_1\_49.pdf},
}



@article{twoway_huang,
author = {Jianhua Z. Huang, Haipeng Shen and Andreas Buja},
title = {The Analysis of Two-Way Functional Data Using Two-Way Regularized Singular Value Decompositions},
journal = {Journal of the American Statistical Association},
volume = {104},
number = {488},
pages = {1609--1620},
year = {2009},
publisher = {ASA Website},

eprint = {

        https://doi.org/10.1198/jasa.2009.tm08024



}

}


@book{statlearning,
  title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  author={Hastie, T. and Tibshirani, R. and Friedman, J.H.},
  isbn={9780387848846},
  lccn={2008941148},
  series={Springer series in statistics},
  year={2009},
  publisher={Springer}
}


@article{silverman1996smoothed,
	author = {Silverman, Bernard W},
	journal = {The Annals of Statistics},
	number = {1},
	pages = {1--24},
	publisher = {Institute of Mathematical Statistics},
	title = {Smoothed functional principal components analysis by choice of norm},
	volume = {24},
	year = {1996}}

@article{Happ_2018,
   title={Multivariate Functional Principal Component Analysis for Data Observed on Different (Dimensional) Domains},
   volume={113},
   ISSN={1537-274X},
   number={522},
   journal={Journal of the American Statistical Association},
   publisher={Informa UK Limited},
   author={Happ, Clara and Greven, Sonja},
   year={2018},
   month=feb, pages={649â€“659} }



@misc{haghbin2023regMFPCA,
      title={Regularized Multivariate Functional Principal Component Analysis},
      author={Hossein Haghbin and Yue Zhao and Mehdi Maadooliat},
      year={2023},
      eprint={2306.13980},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
}

@article{jang2021hpca,
  author    = {Jang, Ji-Hyun},
  title     = {Principal component analysis of hybrid functional and vector data},
  journal   = {Statistics in Medicine},
  volume    = {40},
  number    = {24},
  pages     = {5152--5173},
  year      = {2021},
  month     = {October},
  doi       = {10.1002/sim.9117},
  pmid      = {34160848},
  pmcid     = {PMC9084921},
  note      = {Epub 2021 Jun 23}
}


@misc{HouseholdElectricPower,
	author = {Hebrail,Georges and Berard,Alice},
	howpublished = {UCI Machine Learning Repository},
	title = {{Individual household electric power consumption}},
	year = {2012}}
